{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.domlm import DOMLMConfig\n",
    "\n",
    "config = DOMLMConfig.from_json_file('../domlm-config/config.json')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "dataset_dir = Path('D:/projects/Forage/GenWeb/webpage_information_extraction/dataset/SWDE_Dataset/')\n",
    "domain = 'restaurant'\n",
    "groundtruth_dir = dataset_dir / 'groundtruth' / domain\n",
    "webpages_dir = dataset_dir / 'webpages'\n",
    "domain_dir = webpages_dir / domain\n",
    "website_dir = domain_dir / 'restaurant-frommers(2000)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.domlm.modeling_domlm import DOMLMForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DOMLMForTokenClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(label_files):\n",
    "    label_info = {}\n",
    "    for file in label_files:\n",
    "        label = file.name.split('-')[-1].replace('.txt', '')\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.readlines()\n",
    "            for line in content[2:]:\n",
    "                page_id = line.split('\\t')[0]\n",
    "                if page_id not in label_info:\n",
    "                    label_info[page_id] = {}\n",
    "                nums = line.split('\\t')[1]\n",
    "                value = line.split('\\t')[2].strip()\n",
    "                label_info[page_id][label] = {\n",
    "                    'nums': nums,\n",
    "                    'value': value,\n",
    "                }\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = ['restaurant-frommers-address.txt',\n",
    "               'restaurant-frommers-cuisine.txt',\n",
    "               'restaurant-frommers-name.txt',\n",
    "               'restaurant-frommers-phone.txt']\n",
    "label_files = [groundtruth_dir / l for l in label_files]\n",
    "\n",
    "label_infos = extract_labels(label_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = list(website_dir.glob('*.htm'))[0]\n",
    "html_string = open(path, 'r').read()\n",
    "\n",
    "label2text = label_infos[path.name.split('.')[0]]\n",
    "text2label = {v['value']: {'label':k, 'nums':v['nums']} for k,v in label2text.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.html_utils import get_cleaned_body\n",
    "from src.preprocess import (extract_token_for_nodes, assign_label_for_nodes, generate_subtrees, postorder, _tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tokenizer.model_max_length\n",
    "s = 128\n",
    "\n",
    "padding_idxs = {\n",
    "        \"node_ids\": config.node_pad_id,\n",
    "        \"parent_node_ids\":config.node_pad_id,\n",
    "        \"sibling_node_ids\": config.sibling_pad_id,\n",
    "        \"depth_ids\": config.depth_pad_id,\n",
    "        \"tag_ids\": config.tag_pad_id,\n",
    "        # \"position_ids\": config.pad_token_id,\n",
    "        \"input_ids\": tokenizer.pad_token_id,\n",
    "        \"attention_mask\": 0,\n",
    "        \"labels\": -100\n",
    "    }\n",
    "\n",
    "dom = get_cleaned_body(html_string)\n",
    "token_repr = extract_token_for_nodes(dom)\n",
    "node2label = assign_label_for_nodes(dom, text2label)\n",
    "\n",
    "subtrees = generate_subtrees(dom, token_repr, max_nodes, stride) # requires tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_order = list(dom.iter())\n",
    "post_order = list(postorder(dom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "subtrees = []\n",
    "\n",
    "### init first subtree\n",
    "new = []\n",
    "node_ids = {}\n",
    "for i,el in enumerate(pre_order):\n",
    "    if _tokens_len(new, token_repr) >= m:\n",
    "        break\n",
    "    new.append(el)  \n",
    "    node_ids[el] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RobertaTokenizerFast'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokens_len(new, token_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element tr at 0x1f1b7b01bd0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "while len(new) != 0:\n",
    "    visited = [n for n,idx in node_ids.items() if idx < node_ids[new[0]] ]\n",
    "    total_len = _tokens_len(visited + new, token_repr)\n",
    "    ###prune postorder\n",
    "    for el in post_order:\n",
    "        if el in new or total_len <= m:        \n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                visited.remove(el)\n",
    "                total_len -= _tokens_len([el], token_repr)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    ### prune root\n",
    "    el_last = None\n",
    "    while total_len > m:\n",
    "        if len(visited) != 0:            \n",
    "            el_root = visited[0]                \n",
    "            num_child = sum(child in new or child in visited for child in el_root)        \n",
    "        else:\n",
    "            num_child = 2 # pop from new if there are no nodes in visited left\n",
    "\n",
    "        if num_child < 2:\n",
    "            total_len -= _tokens_len([el_root], token_repr)\n",
    "            visited.pop(0)            \n",
    "        else:\n",
    "            el_last = new.pop()\n",
    "            total_len -= _tokens_len([el_last], token_repr)\n",
    "    t = visited + new\n",
    "    subtrees.append(t)\n",
    "\n",
    "    el_last = new[-1]\n",
    "    ### expand subtree\n",
    "    new = []\n",
    "    next_idx = node_ids[el_last] + 1\n",
    "    for i,el in enumerate(pre_order[next_idx:],start=next_idx):\n",
    "        if _tokens_len(new, token_repr) >= s:\n",
    "            break\n",
    "        new.append(el)        \n",
    "        node_ids[el] = i    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
